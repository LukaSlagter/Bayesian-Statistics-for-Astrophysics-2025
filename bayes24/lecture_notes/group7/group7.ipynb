{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6460d2b9d26b1e20",
   "metadata": {},
   "source": [
    "# Chapter 7: Advantage examples of Bayesian stats. \n",
    "\n",
    "Suggested topics: \n",
    "    - Bayesian Billiards game \n",
    "    - Rejecting outliers with MCMC \n",
    "Idea: create simulation of BB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4d099-5767-43aa-9198-15c39a54d8a1",
   "metadata": {},
   "source": [
    "# Bayesian Billiards\n",
    "A famous thought experiment comparing Bayesian and frequentist estimators is the 'Bayesian billiards'.\n",
    "\n",
    "Suppose we have a rectangular billiard table with length $L$. We throw one black ball on the table, which will bounce around until it stops, at a random $x$-coordinate of the table, and call this $z$, $0\\leq z\\leq L$. For the purposes of this thought experiment, suppose we don't know $z$, and want to estimate it. We could do this by throwing $n$ white balls onto the table, and seeing whether the end up to the left, or to the right of the black ball. By counting how many are to each side, we can reasonably 'guess' the value of $z$. To make the notation easier, suppose $L=1$. \n",
    "\n",
    "Note that one white ball is to the left of the black one with probability $z$, and to the right with probability $1-z$. The random variable \n",
    "$$\n",
    "X_i=\n",
    "\\begin{cases}\n",
    "1 &\\text{ if ball $i$ is to the left}\\\\\n",
    "0 &\\text{if ball $i$ is to the right}\n",
    "\\end{cases}\n",
    "$$\n",
    "is then a Bernoulli random variable with parameter $z$. \n",
    "The probability density is $f_{z}(X_i)=(1-z)^{1-X_i}+z^{X_i}$. \n",
    "The observations $X_1,\\ldots,X_n$ are an i.i.d. sample, and the log-likelihood is\n",
    "$$\\ell(z)\n",
    "=\n",
    "\\log(1-z)(n-\\sum_{i=1}^n X_i)\n",
    "+\\log(z)\\sum_{i=1}^nX_i .\n",
    "$$\n",
    "Setting the derivative of the log-likelihood equal to 0 gives\n",
    "$$\n",
    "\\frac{\\text{d}\\ell(z)}{\\text{d}z}\n",
    "=\n",
    "-\\frac{n-\\sum_{i=1}^n X_i}{1-z}(1-z)\n",
    "+\n",
    "\\frac{\\sum_{i=1}^n X_i}{z}\n",
    "=\n",
    "0,\n",
    "$$\n",
    "which gives us the following the frequentist MLE:\n",
    "$$\n",
    "\\hat z_{\\text{MLE}}\n",
    "=\n",
    "\\frac{\\sum_{i=1}^n X_i}{n}=\\overline{X}.\n",
    "$$\n",
    "So if exactly half of the $n$ white balls lie to the right, the estimate is \n",
    "$\\hat z_{\\text{MLE}}=1/2$,\n",
    "which is to be expected. \n",
    "But if all of the balls lie to the right, the estimate becomes $\\hat z_{\\text{MLE}}=0$, which means that the black ball would lie on the left edge.\n",
    "Intuitively, however, we expect the ball to lie a little to the side, depending on $n$. \n",
    "\n",
    "Now consider the Bayesian framework. \n",
    "The prior of $Z$ is the uniform distribution between 0 and 1, which just has density $\\pi(x)=1$ for all $0\\leq x\\leq1$.\n",
    "(The rest I'll finish later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d9567-0a1a-461c-90a6-d242c877622b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c50329c-16bc-45b8-b804-b005ae15cc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rejecting Outliers with Markov Chain Monte Carlo simulations\n",
    "\n",
    "<b> I will check for spelling/correct wording later :)<b>\n",
    "\n",
    "When doing measurements of a certain physical relation, you would not expect that all points are within one standard deiviation of the theoretical relation. If you assume a big enough sample, we can assume a normal distribution (CLT) thus then the chance of getting a datapoint outside of the 1-$\\sigma$ region is about $32$%, so you would expect about a third of the measurements to be (at least) 1-$\\sigma$ away from the exact relation. However, as we go further and further away from the relation, we expect less and less datapoints there. One of these outliers, a datapoint very far away from the relation, can substantially impact the fit, because it can substatnially change the mean, for example. Hence, there is need to reduce the sensitivty of these outliers. We will do this by removing the outliers, since there is always a change on outliers. Outliers can have multiple causes, for example unmodeled experimental uncertainty or (rare) noice sources, for which aren't always able to account for. One process of removing outliers is by hand. However, as one could image, this is far from ideal, as it can be very subjective and hard to reproduce. \n",
    "\n",
    "Let us make a more systematic way of rejecting outliers. For simpliicity let us look at a straight line. Let $X_1, \\dots, X_n$ be an iid sample, and let $a = (q_1, \\dots, q_n)$ be a set op $n$ binary integers, where $q_i$ is $1$ if the $i$th datapoint is \"good\" and $q_i$ is $0$ if the $i$th datapoint is \"bad\"; an outlier. Furthermore, let $P_b$ be the prior probability that a datapoint is bad and let $(Y_b, V_b)$ be the mean and variance of the distribution of bad points. Note that these extra parameters will be latere marginalized out, we do not need to worry about the fact we have more datapoints than our \"actual\" datapoints. Let $f_g$ be the generative model for the \"good\" datapoints and $f_b$ be the generative model for the \"bad\" datapoints. Then, we can calculate the likelihood: \n",
    "\n",
    "--- let $m, b$ be the parameters of the line, I other info\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i = 1}^n  \\left[f_g(X_1, \\dots X_n | m, b, I)\\right]^{q_i} \\left[f_g(X_1, \\dots X_n | Y_b, V_b, I)\\right]^{1-q_i} \n",
    "$$\n",
    "\n",
    "(since for good datapoints $x * 1$ and for bad $1 * x$ )\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i = 1}^n  \\left[ \\frac{1}{\\sqrt{2\\pi \\left[V_b + \\sigma^2_{X_i}\\right]}} \\exp \\left(-\\frac{[X_i - my_i - b]^2}{2\\sigma_{X_i}^2} \\right)\\right]^{q_i}  \\cdot  \\left[ \\frac{1}{\\sqrt{2\\pi \\left[V_b + \\sigma^2_{X_i}\\right]}} \\exp\\left(-\\frac{[X_i - Y_b]^2}{2[V_b + \\sigma_{X_i}^2]}\\right)\\right]^{1-q_i}\n",
    "$$\n",
    "\n",
    "($y_i$)\n",
    "\n",
    "* p. 11 Hoggs et al: \"the marginalization will require that we have a measure on our parameters (integrals require measures) and that measure is provided by a prior.\" -> marginalization requires Bayesian stats\n",
    "\n",
    "\n",
    "\n",
    "### Sigma Clipping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "used sources:\n",
    "\n",
    "Data analysis recipes: Fitting a model to data, Hogg et al, 2011 (from BS), chapter 3\n",
    "\n",
    "\n",
    "possible sources: \n",
    "\n",
    "https://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html \n",
    "\n",
    "https://www.stat.cmu.edu/technometrics/59-69/VOL-02-02/v0202123.pdf\n",
    "\n",
    "https://d-nb.info/1221556185/34\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
