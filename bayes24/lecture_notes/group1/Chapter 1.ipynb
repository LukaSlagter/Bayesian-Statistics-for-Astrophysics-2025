{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 1: Hypothesis Testing & Maximum Likelihood"
   ],
   "id": "15986c3a895a01a7"
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    "In this chapter we'll explore hypothesis testing.\n",
    "- Hypothesis testing including (Yannick):\n",
    "    - p-values\n",
    "    - pro's and cons\n",
    "- Maximum likelihood including (Sander en Olivier):\n",
    "    - Connection to least-squares (Olivier)\n",
    "    - Connection to generative model (e.g., Hogg et al. article) \n",
    "    - Confidence intervals (\"a frequentist recipe\") (Sander)"
   ],
   "id": "283ccdf132b170ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "A **hypothesis** is a statement regarding a population parameter. The hypothesis test is used to make a decision, based on a sample from the population, which of the hypotheses is true; either the **Nullhypothesis** or the **Alternative hypothesis**.\n",
    "\n",
    "The null hypothesis is the default assumption that there is no relationship between two measured phenomena and is denoted as $H_0$. The alternative hypothesis is the complement of the null hypothesis and is denoted as $H_1$. Let us note this in a more formal mathematical manner.\n",
    "\n",
    "Let $\\theta$ denote the hypothesis and let us partition the parameter space $\\Theta$ into two complementary sets, $\\Theta_0$ and $\\Theta_1$, such that $\\Theta_0$+$\\Theta_1$=$\\Theta$. The **hypothesis test** tests\n",
    "\n",
    "$$H_0:\\ \\theta\\in\\Theta_0\\ \\text{versus}\\ H_1:\\ \\theta\\in\\Theta_1.$$\n",
    "\n",
    "For example, $\\theta$ may characterize the shift in frequency of light due to differences in velocities of an observer and source (Doppler shift). A sceptical astronomer may be interested to see whether the Doppler shift exists or not. The null hypothesis may then be $H_0: \\theta = 0$, implying that there is no shift in frequency while the alternative hypothesis may be $H_1: \\theta \\neq 0$.\n",
    "\n",
    "Sticking with this example, if the sceptical astronomer researches this hypothesis thoroughly (with perfect equipment), they will of course find that $\\theta\\neq0$ since the Doppler effect exists. Hence, the astronomer should **reject** the Null hypothesis. Suppose the sceptical astronomer uses faulty equipment and finds $\\theta=0$, they should then (wrongfully) **retain** the Null hypothesis.\n",
    "\n",
    "This wrongful retention is one of the two errors that can be made in hypothesis testing. Them being a **type I error** and **type II error**. A type I error occurs when the Null hypothesis is rejected when in reality it is true. A type II error occurs when a Null hypothesis is accepted while in reality it is false. In the latter example the astronomer thus makes a type II error, since the Doppler effect is real and thus there *is* a shift in frequency implying that the Null hypothesis should be rejected. These errors have been summarized in the table below.\n",
    "\n",
    "<center>\n",
    "\n",
    "|                       | **Retain Null**   | **Reject Null**   |\n",
    "|-----------------------|-------------------|-------------------|\n",
    "| **H₀ true**           | Correct          | Type I error      |\n",
    "| **H₁ true**           | Type II error    | Correct           |\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "It should be noted that a null hypothesis can never be accepted. For that to happen, an infinite number of measurements must be made which is of course impossible. It is only possible to find evidence *against* the null hypothesis.\n",
    "\n",
    "To test a hypothesis a **Test Statistic (TS)** should be made. The test statistic is then used to measure the probability of a measurement outcome, given that the null hypothesis is *true*. This probability is also known as **the p-value** discussed in the next subsection.\n",
    "\n",
    "The test statistic can be any statistic that combines multiple **random variables** into a single number. This can be for example the median of a dataset, a likelihood ratio, etc. If the probability distribution of the test statistic is known from theory, a probability can be assigned to a given test statistic. If the probability distribution is unknown, **Monte Carlo simulations** can be used assuming $H_0$ is correct. More on this in chapter (HYPERREF NAAR HOOFDSTUK OVER MCMC).\n",
    "\n",
    "## p-values\n",
    "\n",
    "As briefly introduced in the previous subsection, the p-value can be defined as follows:\n",
    "\n",
    "$$\\boxed{\\textit{the p-value is the probability of measuring a dataset $X$ if the null hypothesis is correct.}}$$\n",
    "\n",
    "The p-value is derived from the test statistic which is some function that maps the dataset $X$ to a single value. More precisely, it is derived from the probability distribution of the test statistic known from theory or from a Monte Carlo simulation.\n",
    "\n",
    "The null hypothesis is rejected if the test statistic falls within a predetermined region, called the **rejection region**. Often in literature this rejection region is set at a value for the test statistic, such that the corresponding p-value is less than $0.05$. This limiting value is often called the **significance**.\n",
    "\n",
    "In scientific articles, often something along the lines of \"this null hypothesis is rejected at the $n\\ \\sigma$ level\" is stated. This simply means that the observed result is significant to a degree where the probability of it occurring due to random chance corresponds to a deviation of $n\\sigma$ from the mean of a normal distribution. For example: $1\\ \\sigma$ would correspond to a confidence level of $68.3\\%$, $3\\ \\sigma$ corresponds to $99.7\\%$, etc.\n",
    "\n",
    "## Advantages and Disadvantages of Hypothesis Testing\n",
    "\n",
    "The main advantage of hypothesis testing is that it provides a structured framework for decision-making. The steps of hypothesis testing are as follows:\n",
    "\n",
    "1.  Form a null and alternative hypothesis.\n",
    "\n",
    "2.  Define a test statistic $T$.\n",
    "\n",
    "3.  Find the probability distribution of the test statistic.\n",
    "\n",
    "4.  Compare the test statistic with the probability distribution to find the p-value.\n",
    "\n",
    "In addition, the concept of p-values is widely known across disciplines.\n",
    "\n",
    "Hypothesis testing has some notable downsides. One major issue is that statistical tests are often applied more than necessary. In many situations, confidence intervals or estimation methods might be more appropriate. If you don't have a clear, meaningful hypothesis, don't need to make a definitive yes-or-no decision, or aren't concerned about error probabilities, then hypothesis testing might not be the right approach for your needs.\n",
    "\n",
    "Another issue around p-values is the misinterpretation of them. The p-value is *not* the probability of $H_0$ being correct. This is a mistake often found in articles. It is strictly the probability of measuring a dataset $X$ *if* the null hypothesis is correct based on the defined test statistic (with explicit emphasis on the word *if*). The true definition of the p-value is thus a bit of an awkward one and should therefore be used with caution when communicating with a wider audience consisting of not only those educated in statistics.\n",
    "\n",
    "Scientific fields like psychology and medicine have faced a major challenge called the replication crisis due to this misinterpretation of p-values. Here, many high-profile findings couldn't be reproduced, even with large samples and identical methods. This revealed that a significant number of \"discoveries\\\" might actually be false positives, despite being widely accepted and forming the basis of extensive research. Several factors contribute to this problem, both unintentional and deliberate:\n",
    "\n",
    "1.  Multiple Testing: Many studies fail to account for the fact that testing multiple hypotheses increases the likelihood of false positives. This oversight is often due to inadequate education in statistics.\n",
    "\n",
    "2.  Publication Bias: Journals prioritize significant results, leading to the \"file drawer effect\\\", where studies with non-significant findings remain unpublished. As a result, the scientific record is biased toward positive results.\n",
    "\n",
    "3.  Incentives and Misconduct: Researchers may unintentionally or deliberately engage in practices like:\n",
    "\n",
    "    -   HARKing (Hypothesizing After Results are Known): Creating hypotheses based on the data after seeing the results, which undermines the validity of the conclusions.\n",
    "\n",
    "    -   p-hacking: Manipulating data or analysis (e.g., excluding outliers or changing variables) to achieve statistically significant results.\n",
    "\n",
    "These issues have started to gain attention in recent years, prompting efforts to improve research practices. While fields like astronomy and physics have been less affected, they are not immune. The lesson is clear: small shortcuts or biases can accumulate and harm the integrity of science.\n",
    "\n",
    "## Example: sceptical astronomer"
   ],
   "id": "dca4616875ba712b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Maximum likelihood\n",
    "Before we can talk about the maximum likelihood estimation, we need to take at look at parametric models and parametric inference. A parametric model has the form\n",
    "\n",
    "$$\\mathcal{F} = \\{f(x;\\theta) : \\theta \\in \\Theta\\}$$\n",
    "\n",
    "where $\\Theta \\subset \\mathbb{R}^k$ is the parameter space and $\\theta = (\\theta_1,..., \\theta_k)$ is the parameter, meaning $\\Theta$ is a k-dimensional parameter space with real numbers for each parameters and $\\theta$ is a specific point in this parameter space. The goal of interference is now to estimate the specific value of $\\theta$ that best represents the true parameter $\\theta*$ based on the observed data $X_1,...,X_k$. This specific value of $\\theta$ will be called $\\hat{\\theta}$. To find this $\\hat{\\theta}$, it is natural to ask: \n",
    "$$\\textit{Given observed data $X_1, . . . , X_n$, which value for the true parameter $\\theta^*$ is most probable?}$$\n",
    "\n",
    "However, the problem here is that $\\theta^*$ is a fixed unknown value, while we want to assess it to a probability. This is unfortunately not possible in the frequentist framework. To fix this, we need to use a Bayesian framework and we must ask a different question. \n",
    "\n",
    "$$\\textit{Given a parameter value $\\theta$, how probable is it that we observe the data $X_1, . . . , X_n$?}$$\n",
    "\n",
    "Now the data can be assets probabilistically as they are random variables. The estimator $\\hat{\\theta}$ is then defined as the value of $\\theta$ at which the data is the most likely; so given the parameter, we maximize the likelihood. \n",
    "\n",
    "\n",
    "This maximum likelihood estimation is the most common method for parameter estimation in a parametric model. Before moving forward, we first need to define the likelihood of the data. \n",
    "\n",
    "If we compute Independent and Identically Distributed (IID) values $X_1,...,X_n$ with a PDF $f(x;\\theta)$, the likelihood function is defined by:\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_n(\\theta) =\\prod_{i=1}^{n} f(X_i;\\theta)\n",
    "    \\label{Likeli_func}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the log likelihood function is then defined by: $\\ell_{n}(\\theta)=\\log\\mathcal{L}_n(\\theta)$\n",
    "\n",
    "### Maximum Likelihood Estimator\n",
    "The maximum likelihood estimator (MLE) $\\hat\\theta$ is defined by the value of $\\theta$ that maximizes $\\mathcal{L}_n(\\theta)$. This value of $\\hat\\theta$ is the same for both $\\mathcal{L}_n(\\theta)$ and $\\ell_{n}(\\theta)$, as the maximum of a function occurs at the same place as the maximum of the logarithm of this function. This is especially convenient if we look at the maximum likelihood estimator in a mathematical way. The MLE of the likelihood is defined with a product (\\ref{likeli_est}). \n",
    "\\begin{equation}\n",
    "    \\hat{\\theta} = \\arg \\max_{\\theta \\in \\Theta} \\prod_{i=1}^{n} f_{\\theta}(X_i).\n",
    "    \\label{likeli_est}\n",
    "\\end{equation}\n",
    "So the MLE of the log-likelihood is then defined with a sum (\\ref{loglikeli_est}).\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\sum_{i=1}^{n} \\ln f_{\\theta}(X_i)\n",
    "    \\label{loglikeli_est}\n",
    "\\end{equation}\n",
    "### Least-squares\n",
    "### Generative model\n",
    "Imagine you have a data set \\textit{y} of $N$ data points, which you want to be able to explain using a model. We expect that the data comes from a straight line model, \\begin{equation}\n",
    "    f(x)=ax+b.\n",
    "\\end{equation}\n",
    "This model produces the data set \\{$y_i$\\} with uncertainties \\{$\\sigma_{y_i}$\\}. Our goal is to find the parameters that are most likely to have generated our data. We can do this by writing down a \\textit{generative model}. This model should be a description of a statistical procedure that could generate the known data set. For this example, we say that the \\{$y_i$\\} are drawn from a Gaussian distribution with zero mean and uncertainties \\{$\\sigma_{y_i}$\\}. So the data deviates from a straight line with small offsets drawn from a Gaussian distribution. Now we can create a distribution which describes the probability that we get points $y_i$, given the knowledge that we have: \\begin{equation}\n",
    "    p(y_i|x_i,\\sigma_i,a,b)=\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{-\\frac{y_i-ax_i-b}{2\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where \\{$x_i$\\} are the independent positions, $a$ is the slope, $b$ the intercept and $y_i$ and $\\sigma_{y_i}$ as before. Now that we have this distribution, we can achieve our goal of finding the parameters that are most likely to have generated our data. We can do this by writing down the likelihood of the complete set \\textit{y} given our prior knowledge. \\begin{equation}\n",
    "    \\mathscr{L}=\\prod_{i=1}^Np(y_i|x_i,\\sigma_i,a,b).\n",
    "\\end{equation}\n",
    "### Confidence intervals\n",
    "\n",
    "One of the most popular ways to communicate uncertainty is through *confidence intervals*. The idea is to find an interval around the estimated value $\\hat\\theta$ that covers the true value $\\theta^*$. We characterize the confidence interval through the *confidence level* $\\gamma$. A $\\gamma$-confidence interval has a probability of atleast $\\gamma$ that the interval $(\\hat\\theta_l,\\hat\\theta_r)$ contains the true parameter $\\theta^*$, or in mathematical terms: \\begin{equation} P(\\theta^*\\in(\\hat\\theta_l,\\hat\\theta_r)\\geqq\\gamma \\end{equation}\n",
    "\n"
   ],
   "id": "db3413cc7142bbe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e7b4373b9e68ec8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Sources:**\n",
    "- Hogg et al. (2010) Data analysis recipes: Fitting a model to data\n",
    "- Statistical inference, Casella & Berger (2002)\n",
    "- Mathematical Statistics with Applications, Wackerly et al. (2008)\n",
    "- All of Statistics, Wasserman (2004)\n"
   ],
   "id": "97e745ffddb19d6d"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T13:02:41.051333Z",
     "start_time": "2024-11-05T13:02:41.047414Z"
    }
   },
   "id": "258f787934659727",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
